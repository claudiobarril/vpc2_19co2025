{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V26TaBJEPA5F"
   },
   "outputs": [],
   "source": [
    "!pip install torchmetrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import cv2 as cv\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score,confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np"
   ],
   "metadata": {
    "id": "SSKIk1NUPD4W"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#TRAER LOS DATOS DESDE KAGGLE\n",
    "import kagglehub\n",
    "abdallahalidev_plantvillage_dataset_path = kagglehub.dataset_download('abdallahalidev/plantvillage-dataset')\n",
    "\n",
    "print('Data source import complete.')\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ],
   "metadata": {
    "id": "wj-f-cNWPFMi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tratamiento de datos"
   ],
   "metadata": {
    "id": "VhB3nlLJPKVr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Species mapping\n",
    "species_es_map = {\n",
    "    'Strawberry': 'Fresa',\n",
    "    'Grape': 'Uva',\n",
    "    'Potato': 'Papa',\n",
    "    'Blueberry': 'Arándano',\n",
    "    'Corn_(maize)': 'Maíz',\n",
    "    'Tomato': 'Tomate',\n",
    "    'Peach': 'Durazno',\n",
    "    'Pepper,_bell': 'Pimiento',\n",
    "    'Orange': 'Naranja',\n",
    "    'Cherry_(including_sour)': 'Cereza',\n",
    "    'Apple': 'Manzana',\n",
    "    'Raspberry': 'Frambuesa',\n",
    "    'Squash': 'Calabaza',\n",
    "    'Soybean': 'Soja'\n",
    "}\n",
    "\n",
    "# Disease mapping\n",
    "disease_es_map = {\n",
    "    'Black_rot': 'Podredumbre negra',\n",
    "    'Early_blight': 'Tizón temprano',\n",
    "    'Target_Spot': 'Mancha diana',\n",
    "    'Late_blight': 'Tizón tardío',\n",
    "    'Tomato_mosaic_virus': 'Virus del mosaico',\n",
    "    'Haunglongbing_(Citrus_greening)': 'Huanglongbing (enverdecimiento de los cítricos)',\n",
    "    'Leaf_Mold': 'Moho de la hoja',\n",
    "    'Leaf_blight_(Isariopsis_Leaf_Spot)': 'Tizón de la hoja',\n",
    "    'Powdery_mildew': 'Oídio',\n",
    "    'Cedar_apple_rust': 'Roya del manzano y cedro',\n",
    "    'Bacterial_spot': 'Mancha bacteriana',\n",
    "    'Common_rust_': 'Roya común',\n",
    "    'Esca_(Black_Measles)': 'Esca',\n",
    "    'Tomato_Yellow_Leaf_Curl_Virus': 'Virus del rizado amarillo de la hoja',\n",
    "    'Apple_scab': 'Sarna',\n",
    "    'Northern_Leaf_Blight': 'Tizón foliar del norte',\n",
    "    'Spider_mites Two-spotted_spider_mite': 'Ácaros araña de dos manchas',\n",
    "    'Septoria_leaf_spot': 'Mancha foliar por septoria',\n",
    "    'Cercospora_leaf_spot Gray_leaf_spot': 'Mancha foliar por cercospora / mancha foliar gris',\n",
    "    'Leaf_scorch': 'Quemadura de la hoja'\n",
    "}"
   ],
   "metadata": {
    "id": "IEGFAEs8PIo0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Directorio en Kaggle\n",
    "# dataset_path = \"plantvillage_dataset/plantvillage dataset/color\"\n",
    "# Directorio para corrida local\n",
    "dataset_path = \"/kaggle/input/plantvillage-dataset/plantvillage dataset/color/\"\n",
    "\n",
    "data = []\n",
    "\n",
    "# Recorremos carpetas y archivos\n",
    "if os.path.exists(dataset_path):\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        species, disease = folder.split('___', 1)\n",
    "        healthy = disease == 'healthy'\n",
    "        disease = None if healthy else disease\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                data.append({\n",
    "                    'Format': 'color',\n",
    "                    'Species': species,\n",
    "                    'Healthy': healthy,\n",
    "                    'Disease': disease,\n",
    "                    'FileName': file\n",
    "                })\n",
    "\n",
    "# Crear DataFrame\n",
    "df = pd.DataFrame(data)"
   ],
   "metadata": {
    "id": "a6OTaZ6IPOSj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Agregar nombre en español para especie\n",
    "df['Especie'] = df['Species'].map(species_es_map)\n",
    "\n",
    "# Agregar nombre en español para enfermedad (o 'Sano' si es healthy)\n",
    "df['Enfermedad'] = df.apply(\n",
    "    lambda row: 'Sano' if row['Healthy'] else disease_es_map.get(row['Disease'], row['Disease']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Duplicar columna \"Enfermedad\" como \"Label\", por cuestiones prácticas\n",
    "df['Label'] = df['Enfermedad']\n",
    "\n",
    "# Codificar las etiquetas compuestas como números\n",
    "df['Label_id'] = df['Label'].astype('category').cat.codes\n",
    "\n",
    "# Crear el diccionario de mapeo id → etiqueta compuesta\n",
    "label_map = dict(enumerate(df['Label'].astype('category').cat.categories))\n",
    "\n",
    "# Número de clases únicas\n",
    "NUM_CLASSES = len(label_map)"
   ],
   "metadata": {
    "id": "NSry9gf8POq_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Número de clases: {NUM_CLASSES}\")"
   ],
   "metadata": {
    "id": "U9ph3by0PX0Q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df[\"disease_text\"] = df.apply(lambda r: \"healthy\" if r[\"Healthy\"] else r[\"Disease\"], axis=1)\n",
    "df[\"Label_id\"] = df[\"disease_text\"].astype(\"category\").cat.codes\n",
    "NUM_CLASSES = df[\"Label_id\"].nunique()\n",
    "label_map = dict(enumerate(df[\"disease_text\"].astype(\"category\").cat.categories))\n"
   ],
   "metadata": {
    "id": "WgtulH59PaS_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df"
   ],
   "metadata": {
    "id": "5Z1ndlD0PbzH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Semilla reproducible\n",
    "SEED = 42\n",
    "\n",
    "# Split 85% train, 15% valid\n",
    "train_df, valid_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.15,\n",
    "    stratify=df['Label_id'],\n",
    "    random_state=SEED\n",
    ")"
   ],
   "metadata": {
    "id": "tChzkJokPdXp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "len(valid_df)"
   ],
   "metadata": {
    "id": "GY0ExNSAPe8H"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clase"
   ],
   "metadata": {
    "id": "gH_ajrOtPfQs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class PlantVillageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, root_dir, format_type='color', transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.root_dir = root_dir\n",
    "        self.format_type = format_type\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        folder = f\"{row['Species']}___{'healthy' if row['Healthy'] else row['Disease']}\"\n",
    "        image_path = os.path.join(self.root_dir, folder, row['FileName'])\n",
    "\n",
    "        image = cv.imread(image_path, cv.IMREAD_COLOR)  # lectura robusta en color\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found or unreadable: {image_path}\")\n",
    "\n",
    "        # Asegurar RGB (imread devuelve BGR)\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "        # image es numpy array uint8, listo para ToPILImage\n",
    "        label = torch.tensor(row['Label_id'], dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ],
   "metadata": {
    "id": "KDNzpQw6Pf_4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dividir dataset para iterar rapido"
   ],
   "metadata": {
    "id": "W1RDrhCNPtft"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "SE_SMALL_TRAIN = True\n",
    "TRAIN_FRAC = 0.2  # porcentaje de train\n",
    "\n",
    "def stratified_fraction_indices(y, frac=0.2, seed=1337):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=frac, random_state=seed)\n",
    "    (idx_small, _), = sss.split(np.zeros_like(y), y)  # ahora train_size=frac\n",
    "    return idx_small\n",
    "\n",
    "base_train_df = train_df\n",
    "if USE_SMALL_TRAIN:\n",
    "    idx = stratified_fraction_indices(train_df[\"Label_id\"].values, frac=TRAIN_FRAC, seed=1337)\n",
    "    base_train_df = train_df.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "total_train = len(train_df)\n",
    "subset_train = len(base_train_df)\n",
    "\n",
    "print(f\"Total imágenes en train: {total_train}\")\n",
    "print(f\"Usando en subset: {subset_train} ({subset_train/total_train:.1%} del train)\")"
   ],
   "metadata": {
    "id": "LGGI5HJ-PkM5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "id": "sYL_uAIAPxYI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Normalización estándar ImageNet\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomResizedCrop((224, 224), scale=(0.5, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.03),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.25,\n",
    "                             scale=(0.02, 0.12),\n",
    "                             ratio=(0.3, 3.3)),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "])\n",
    "\n",
    "\n",
    "# Transform para validación\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "])\n"
   ],
   "metadata": {
    "id": "1qVZfnpYPwEP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = PlantVillageDataset(base_train_df, dataset_path, transform=train_transform)\n",
    "valid_dataset = PlantVillageDataset(valid_df,        dataset_path, transform=val_transform)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "data_dict = {\"train\": train_loader, \"valid\": valid_loader, \"image_width\": 224, \"image_height\": 224}\n"
   ],
   "metadata": {
    "id": "t1vvqZhZPzli"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")"
   ],
   "metadata": {
    "id": "1kRhQkLYP07d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class weight"
   ],
   "metadata": {
    "id": "nRwSCMYAP2fu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "counts = base_train_df[\"Label_id\"].value_counts().sort_index().to_numpy(dtype=np.float32)\n",
    "class_weights = torch.tensor(1.0 / (counts + 1e-6), dtype=torch.float32)\n",
    "class_weights = class_weights / class_weights.sum() * len(counts)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n"
   ],
   "metadata": {
    "id": "mukLpDfAP3wy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "zI6GskJ7P6Jm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "id": "1jWyoh4gP8OW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)"
   ],
   "metadata": {
    "id": "1InXbiUNP8zp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import cv2 as cv\n",
    "cv.setNumThreads(0)   # Esto lo agregue porque me tiraba un bug"
   ],
   "metadata": {
    "id": "kVjkly5pP-ux"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Info de loaders e imagen\n",
    "data_dict = {\n",
    "    \"train\": train_loader,\n",
    "    \"valid\": valid_loader,\n",
    "    \"image_width\": 224,\n",
    "    \"image_height\": 224\n",
    "}\n",
    "\n",
    "# TensorBoard\n",
    "writer = {\n",
    "    \"train\": SummaryWriter(log_dir=\"runs/plant_train\"),\n",
    "    \"valid\": SummaryWriter(log_dir=\"runs/plant_valid\")\n",
    "}"
   ],
   "metadata": {
    "id": "bN_aYkJaQAxz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train (cambie la funcion porque me tiraba errores)"
   ],
   "metadata": {
    "id": "UGe4QCzsQFnb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.amp import autocast, GradScaler  # en lugar de torch.cuda.amp\n",
    "scaler = GradScaler()"
   ],
   "metadata": {
    "id": "3C5G3ZwiQIeg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    metric,  # torchmetrics.Metric\n",
    "    data: Dict[str, Any],\n",
    "    epochs: int,\n",
    "    tb_writer: Optional[Dict[str, Any]] = None,  # {\"train\": SummaryWriter, \"valid\": SummaryWriter}\n",
    "    log_interval: int = 10,\n",
    "    early_stop_patience: int = 3,\n",
    "    grad_clip_norm: Optional[float] = None,\n",
    "    use_amp: bool = True,\n",
    "    best_ckpt_path: str = \"mejor_modelo.pth\",\n",
    ") -> Dict[str, list]:\n",
    "    \"\"\"\n",
    "    Entrena un modelo de clasificación con validación, AMP, early stopping y TensorBoard.\n",
    "\n",
    "    data: {\n",
    "        \"train\": DataLoader,\n",
    "        \"valid\": DataLoader,\n",
    "        \"image_width\": int,\n",
    "        \"image_height\": int\n",
    "    }\n",
    "    \"\"\"\n",
    "    train_loader = data[\"train\"]\n",
    "    valid_loader = data[\"valid\"]\n",
    "    image_width, image_height = data[\"image_width\"], data[\"image_height\"]\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    device_type = \"cuda\" if torch.cuda.is_available() and device.type == \"cuda\" else \"cpu\"\n",
    "\n",
    "    # AMP moderno\n",
    "    scaler = GradScaler(enabled=(use_amp and device_type == \"cuda\"))\n",
    "\n",
    "    # TB graph (opcional; a veces falla con ciertos módulos)\n",
    "    if tb_writer:\n",
    "        try:\n",
    "            dummy = torch.zeros((1, 3, image_height, image_width), device=device)\n",
    "            tb_writer[\"train\"].add_graph(model, dummy)\n",
    "        except Exception:\n",
    "            pass  # evitar que rompa por modelos no trazables\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"valid_loss\": [],\n",
    "        \"valid_acc\": [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # -----------------------\n",
    "        # Train\n",
    "        # -----------------------\n",
    "        model.train()\n",
    "        metric.reset()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch}/{epochs} [Train]\")\n",
    "        for step, (x, y) in pbar:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            if use_amp:\n",
    "                with autocast(device_type=device_type):\n",
    "                    logits = model(x)\n",
    "                    loss = criterion(logits, y)\n",
    "            else:\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "\n",
    "            if scaler.is_enabled():\n",
    "                scaler.scale(loss).backward()\n",
    "                if grad_clip_norm is not None:\n",
    "                    # unscale antes de clipear\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                if grad_clip_norm is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            metric.update(logits, y)\n",
    "\n",
    "            if step % log_interval == 0:\n",
    "                avg_loss = running_loss / (step + 1)\n",
    "                batch_acc = metric.compute().item()\n",
    "                pbar.set_postfix(loss=f\"{avg_loss:.4f}\", acc=f\"{batch_acc:.4f}\")\n",
    "\n",
    "        epoch_train_loss = running_loss / max(1, len(train_loader))\n",
    "        epoch_train_acc = metric.compute().item()\n",
    "        history[\"train_loss\"].append(epoch_train_loss)\n",
    "        history[\"train_acc\"].append(epoch_train_acc)\n",
    "\n",
    "        # -----------------------\n",
    "        # Valid\n",
    "        # -----------------------\n",
    "        model.eval()\n",
    "        metric.reset()\n",
    "        val_running_loss = 0.0\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            for x, y in valid_loader:\n",
    "                x = x.to(device, non_blocking=True)\n",
    "                y = y.to(device, non_blocking=True)\n",
    "                if use_amp:\n",
    "                    with autocast(device_type=device_type):\n",
    "                        logits = model(x)\n",
    "                        loss = criterion(logits, y)\n",
    "                else:\n",
    "                    logits = model(x)\n",
    "                    loss = criterion(logits, y)\n",
    "\n",
    "                val_running_loss += loss.item()\n",
    "                metric.update(logits, y)\n",
    "\n",
    "        epoch_val_loss = val_running_loss / max(1, len(valid_loader))\n",
    "        epoch_val_acc = metric.compute().item()\n",
    "        history[\"valid_loss\"].append(epoch_val_loss)\n",
    "        history[\"valid_acc\"].append(epoch_val_acc)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch} | \"\n",
    "            f\"Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | \"\n",
    "            f\"Valid Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # TensorBoard\n",
    "        if tb_writer:\n",
    "            tb_writer[\"train\"].add_scalar(\"loss\", epoch_train_loss, epoch)\n",
    "            tb_writer[\"train\"].add_scalar(\"accuracy\", epoch_train_acc, epoch)\n",
    "            tb_writer[\"valid\"].add_scalar(\"loss\", epoch_val_loss, epoch)\n",
    "            tb_writer[\"valid\"].add_scalar(\"accuracy\", epoch_val_acc, epoch)\n",
    "            tb_writer[\"train\"].flush()\n",
    "            tb_writer[\"valid\"].flush()\n",
    "\n",
    "        # Early stopping + checkpoint\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), best_ckpt_path)\n",
    "            print(f\"✅ Mejor modelo guardado en '{best_ckpt_path}' (val_loss={best_val_loss:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stop_patience:\n",
    "                print(f\"⏹️ Early stopping en epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    return history\n"
   ],
   "metadata": {
    "id": "edM_wUUlQTbx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tecnica entrenar multiple pasos"
   ],
   "metadata": {
    "id": "vRB5ow7-QVAU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paso A"
   ],
   "metadata": {
    "id": "RUOQX44LQWf7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Congelar backbone para linear probe\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "# 2) Optimizer (solo la cabeza)\n",
    "optimizer = torch.optim.AdamW(model.fc.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# 3) Métrica (macro)\n",
    "metric = torchmetrics.classification.MulticlassAccuracy(\n",
    "    num_classes=NUM_CLASSES, average='macro'\n",
    ").to(device)\n",
    "\n",
    "history_s1 = train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    metric=metric,\n",
    "    data=data_dict,\n",
    "    epochs=10,\n",
    "    tb_writer=writer,\n",
    "    best_ckpt_path=\"best_s1.pth\"\n",
    ")"
   ],
   "metadata": {
    "id": "9jGRM2zqQXiV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paso B"
   ],
   "metadata": {
    "id": "axC5CLlDQYpa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.load_state_dict(torch.load(\"/content/mejor_modelo.pth\", map_location=device))"
   ],
   "metadata": {
    "id": "pWVvdpnSQZgW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# PASO 2\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in model.layer4.parameters():\n",
    "    p.requires_grad = True\n",
    "for m in model.layer4.modules():  # BN de layer4 en train\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        m.train()\n",
    "        m.requires_grad_(True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=3e-4, weight_decay=1e-4\n",
    ")\n",
    "\n",
    "history_s2 = train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    metric=metric,\n",
    "    data=data_dict,\n",
    "    epochs=10,  # 8–12\n",
    "    tb_writer=writer,\n",
    "    best_ckpt_path=\"best_s2.pth\"\n",
    ")\n"
   ],
   "metadata": {
    "id": "5N5CzbtrQbNL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Paso C"
   ],
   "metadata": {
    "id": "x8IJsBjKQceL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.load_state_dict(torch.load(\"best_s2.pth\", map_location=device))\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "history_s3 = train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    metric=metric,\n",
    "    data=data_dict,\n",
    "    epochs=12,  # 10–15\n",
    "    tb_writer=writer,\n",
    "    best_ckpt_path=\"best_s3.pth\"\n",
    ")\n"
   ],
   "metadata": {
    "id": "rHYWKH88QdMh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Validacion"
   ],
   "metadata": {
    "id": "7kIPuDL5Qeid"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def eval_metrics(model, valid_loader, device, label_map=None):\n",
    "    \"\"\"\n",
    "    Devuelve:\n",
    "      acc_global, recall_sano, f1_sano, recall_enf, f1_enf\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for x, y in valid_loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        y_true.append(y.cpu().numpy())\n",
    "        y_pred.append(pred.cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    # 1) Accuracy global (multiclase)\n",
    "    acc_global = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # 2) Identificar el id de 'healthy' (Sano)\n",
    "    #    preferimos usar label_map si lo tenés (id->nombre)\n",
    "    if label_map is not None:\n",
    "        healthy_id = None\n",
    "        for k, v in label_map.items():\n",
    "            if str(v).lower() == \"healthy\":\n",
    "                healthy_id = int(k); break\n",
    "        if healthy_id is None:\n",
    "            raise ValueError(\"No encontré 'healthy' en label_map. Revisá label_map.\")\n",
    "    else:\n",
    "        # fallback: deducir desde el y_true (si df está accesible podrías usarlo mejor)\n",
    "        # acá asumimos que existe al menos una etiqueta 'healthy' en valid\n",
    "        raise ValueError(\"Pasá label_map para identificar la clase 'healthy' de forma segura.\")\n",
    "\n",
    "    # 3) Métricas de la clase Sano (one-vs-rest)\n",
    "    y_true_sano = (y_true == healthy_id).astype(int)\n",
    "    y_pred_sano = (y_pred == healthy_id).astype(int)\n",
    "    recall_sano = recall_score(y_true_sano, y_pred_sano, zero_division=0)\n",
    "    f1_sano     = f1_score(y_true_sano, y_pred_sano, zero_division=0)\n",
    "\n",
    "    # 4) Métricas de Enfermedades combinadas (todo lo que NO es healthy)\n",
    "    y_true_enf = (y_true != healthy_id).astype(int)\n",
    "    y_pred_enf = (y_pred != healthy_id).astype(int)\n",
    "    recall_enf = recall_score(y_true_enf, y_pred_enf, zero_division=0)\n",
    "    f1_enf     = f1_score(y_true_enf, y_pred_enf, zero_division=0)\n",
    "\n",
    "    return acc_global, recall_sano, f1_sano, recall_enf, f1_enf\n",
    "\n",
    "# --- Llamada y print bonito ---\n",
    "acc, r_sano, f1_sano, r_enf, f1_enf = eval_metrics(model, valid_loader, device, label_map=label_map)\n",
    "print(f\"Accuracy global: {acc:.4f}\")\n",
    "print(f\"Recall clase 'Sano': {r_sano:.4f}\")\n",
    "print(f\"F1-score clase 'Sano': {f1_sano:.4f}\")\n",
    "print(f\"Recall enfermedades (combinadas): {r_enf:.4f}\")\n",
    "print(f\"F1-score enfermedades (combinadas): {f1_enf:.4f}\")"
   ],
   "metadata": {
    "id": "GxUi4swtQg98"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def preds_targets(model, loader, device):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for x,y in loader:\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        p = model(x).argmax(1)\n",
    "        ys.append(y.cpu().numpy()); ps.append(p.cpu().numpy())\n",
    "    return np.concatenate(ys), np.concatenate(ps)\n",
    "\n",
    "y_true, y_pred = preds_targets(model, valid_loader, device)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion matrix shape:\", cm.shape)"
   ],
   "metadata": {
    "id": "yAQTMp6vQtw-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class_names = sorted(train_df['disease_text'].unique().tolist())\n"
   ],
   "metadata": {
    "id": "fI2Ayu6sQy3S"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))"
   ],
   "metadata": {
    "id": "dekvoINCQvuP"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
